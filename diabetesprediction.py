# -*- coding: utf-8 -*-
"""DiabetesPrediction.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/github/NathanBrunet/ML-Project-Diabetes-Prediction/blob/main/DiabetesPrediction.ipynb

<center><h1><b></b></h1></center>
<center><h1><b>DSTI
<center><h1><b>Machine Learning with Python Labs</b></h1></center>
<center><h3><b>BRUNET Nathan - IBITOWA Abraham - HAOUA Anis Sofiane - KAKY SUZY Joelly Magalie - NIANG Falilou</b></h3></center><center><h1><b>Diabetes Prediction System</b></h1></center>

## Data Loading, Preprocessing & Exploratory Data Analysis (EDA)
"""

from google.colab import drive
drive.mount('/content/drive')

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from IPython.display import display

# Load the dataset
df_raw = pd.read_csv("https://raw.githubusercontent.com/NathanBrunet/ML-Project-Diabetes-Prediction/refs/heads/main/TAIPEI_diabetes.csv")

# Make a copy of the DataFrame for modifications
df = df_raw.copy()

# General statistics and overview
print("Number of columns:")
print(df.shape[1])
print("First 5 columns of DF:")
display(df.head())
print(" Last 5 columns of DF: ")
display(df.tail())

# Remove the first column which brings no information

def remove_patient_id(dataframe):
    """
    Function to remove the first column of the DataFrame (PatientID which is useless).
    """
    df_noid = dataframe.drop(columns=['PatientID']) # No ID
    return df_noid

df_noid = remove_patient_id(df)
display(df_noid.describe())

"""No variable at 0 as minimums except for Pregnancies which seems logical: a good sign (to comment).
Variables not on the same scales: need to normalize later (or standardize).
"""

# Check more surely for potential NaN values
na_values = df_noid.isna().sum()
print("Number of NaN per column :")
print(na_values)

# Check each column type
print(" Column types: ")
print(df_noid.dtypes)

# Check for duplicates in the DataFrame
num_duplicates = df_noid.duplicated().sum()

# Print the number of duplicates
print(f'Number of duplicates: {num_duplicates}')

# Remove the duplicates in place
df_noid.drop_duplicates(inplace=True)

# Verify that duplicates were removed
print(f'Number of rows after removing duplicates: {df_noid.shape[0]}')

# Count the number of people with and without diabetes
diabetes_counts = df_noid['Diabetic'].value_counts()

# Create a pie chart to visualize the distribution
plt.figure(figsize=(8, 6))
plt.pie(diabetes_counts, labels=['Non-Diabetic', 'Diabetic'], autopct='%1.1f%%', colors=['lightblue', 'lightcoral'])
plt.title('Diabetic vs Non-Diabetic Distribution')
plt.show()

"""As we can see, there is an imbalance in favor of the non-diabetic class. That's not extreme, but it's there. In reality, outside of our dataset, such an imbalance is not outrageous, since diabetes only affects about 11-12% of people worldwide (cf. study). Hence, we do not see a true need for resampling techniques (SMOTE, etc.)"""

df_noid.hist(bins=60, figsize=(15, 10))
plt.show()

# Examination of relations between features and target variable

import matplotlib.pyplot as plt
import seaborn as sns

def target_density_estimation(df, target):
    """
    Function to plot the target class distribution (Diabetic vs Non-diabetic)
    for each numerical column in the DataFrame.
    """
    for col in df.columns:
        if col == target:
            continue

        plt.figure(figsize=(10, 5))

        sns.kdeplot(df[df[target] == 1][col], color="green", fill=True, label='Has Diabetes')  # Changed shade=True to fill=True
        sns.kdeplot(df[df[target] == 0][col], color="red", fill=True, label='Has no Diabetes')  # Changed shade=True to fill=True

        plt.legend()
        plt.xlim(-10, 150)
        plt.title(f"Diabetic Density of {col}")
        plt.show()

target_density_estimation(df_noid, "Diabetic")

"""<p style="text-align: justify;">More globally for our data, features/variables are of homogeneous types, there appears to be no NaN values,duplicates, or problematic null values (such as for PlasmaGlucose or TricepsThickness). The fact that there are 2 float type variables (BMI, DiabetesPedigree) as well as int type ones is not an issue for numerical analysis.</p>"""

# Dataframe with no target variable (wt = "without target") boxplots

def remove_diabetic(dataframe) :
    """
    Function to remove the target variable from the DataFrame.
    """
    df_wt = dataframe.drop(columns=['Diabetic'])
    return df_wt

df_wt = remove_diabetic(df_noid)

# Create BoxPlots to visualize distributions and potential outliers

def plot_boxplots(df, cols, figsize=(14, 8), grid=(2, 4), color='lightblue'):
    """
    Generate boxplots of features for data visualization.
    """
    plt.figure(figsize=figsize)

    for i, col in enumerate(cols, 1):
        plt.subplot(grid[0], grid[1], i)
        sns.boxplot(data=df, x=col, color=color)
        plt.title(f'Boxplot of {col}')
        plt.tight_layout()

    plt.show()

plot_boxplots(df_wt, df_wt.columns)

"""There seems to be some outliers in our dataset to handle (proceed case by case for each column, comment : high SerumInsulin is normal in this context for instance).

We will manage this before creating additional features so that it doens't contaminate our feature engineering processes.
"""

# Outliers management

def winsorize_with_exception(df, lower_percentile=0.05, upper_percentile=0.95):
    """
    Winsorize columns of the DataFrame by replacing extreme values.
    """
    outlier_columns = {
        'PlasmaGlucose': {'lower': True, 'upper': False},
        'DiastolicBloodPressure': {'lower': True, 'upper': False},
        'TricepsThickness': {'lower': False, 'upper': True},
        'SerumInsulin': {'lower': True, 'upper': True},
        'BMI': {'lower': False, 'upper': True}
    }

    for col, thresholds in outlier_columns.items():
        lower_threshold = df[col].quantile(lower_percentile) if thresholds['lower'] else None
        upper_threshold = df[col].quantile(upper_percentile) if thresholds['upper'] else None

        if lower_threshold is not None:
            df[col] = df[col].clip(lower=lower_threshold)
        if upper_threshold is not None:
            df[col] = df[col].clip(upper=upper_threshold)

    return df

df_wt=winsorize_with_exception(df_wt)
print("General statistics before winsorization:")
display(df_noid.describe())
print("General statistics after winsorization:")
display(df_wt.describe())

"""https://practicetransformation.umn.edu/practice-tools/recognizing-medial-crisis-diabetes/ =>  No need to handle outliers for PlasmaGlucose for the upper threshold (but we need it for lower threshold)

https://www.heart.org/en/health-topics/high-blood-pressure/understanding-blood-pressure-readings => Same for DiastolicBloodPressure (lower threshold to handle : 60 mmHg is low : https://www.uab.edu/news/research-innovation/diastolic-blood-pressure-how-low-is-too-low)

https://pmc.ncbi.nlm.nih.gov/articles/PMC9127233/ => Need to winsorize for upper threshold of TricepsThickness

https://www.nhsinform.scot/healthy-living/food-and-nutrition/healthy-eating-and-weight-management/body-mass-index-bmi/ => Need it for upper threshold of BMI

https://blog.knockdiabetes.com/insulin-resistance-and-insulin-sensitivity/ => Need to manage lower and upper thresholds for SerumInsulin

DiabetesPedigree => No contradictory data was found concerning the range obtained for this feature.
"""

# Compute the correlation matrix

# Correlation matrix before any treatments (except outliers management)
temp = df_wt.copy()
temp['Diabetic'] = df_noid['Diabetic']
corr_matrix_temp = temp.corr()
plt.figure(figsize=(10, 8))
sns.heatmap(corr_matrix_temp, annot=True, cmap='coolwarm', fmt='.2f', linewidths=0.5)
plt.title('Correlation Matrix after Outliers Management (no other treatments)')
plt.show()

"""We will now proceed to the feature engineering step before any additional treatments to the data since we want to apply the transformations that will follow to all the data we will generate and have in hand in one go.

" 3.2 Wrapper based feature selection
Wrapper-based feature selection is a ML technique that approaches the process of selecting subsets of features as a search problem. This method involves training and evaluating the effectiveness of ML models with various feature subsets to identify the one that provides the best prediction performance. Wrapper methods evaluate feature subsets by applying a specific ML algorithm as a “wrapper” around the feature selection process. "

https://www.frontiersin.org/journals/artificial-intelligence/articles/10.3389/frai.2024.1421751/full

## Feature Engineering
"""

# Feature Engineering

def generate_features(df):
    """
    Generate new features to enrich the dataset and explore hidden relevant patterns beteween existing ones.
    """
    # Numerical Features
    df["Pseudo_Inverse_MI"] = (df["PlasmaGlucose"] * df["SerumInsulin"])/10000 # Pseudo Inverse Matsuda Index to investigate further the role of PlasmaGlucose and insulin resistance

    # HOMA-IR and QUICKI were considered but abandoned since (resp.) inversely proportional/(resp.) proportional  to Matsuda Index
    # HOMA-B was considered but abandoned because it can generate NaN/inf values

    df["BMI_DiabetesPedigree"] = df["BMI"] * df["DiabetesPedigree"] # To investigate further the potential role of diabetes genetic predispositions
    df['DBP_Age_Freq_Score'] = df['DiastolicBloodPressure'] / (df['Age'] * df['Age'].map(df['Age'].value_counts(normalize=True))) # To assess a patient's relative cardiometabolic risk, correcting for age distribution bias in the cohort.

    # Glucose/Insulin Ratio was considered but left over (correlation with Pseudo_Inverse_MI)

    # Categorical Features
    df["Age_Category"] = pd.cut(df["Age"], bins=[18, 30, 60, np.inf], labels=["Young", "Adult", "Senior"])
    df["BMI_Category"] = pd.cut(df["BMI"], bins=[18, 25, 30, np.inf], labels=["Normal", "Overweight", "Obese"])
    df["Glucose_Category"] = pd.cut(df["PlasmaGlucose"], bins=[0, 70, 130, np.inf], labels=["Hypoglycemia", "Normal", "High"])
    df["BloodPressure_Category"] = pd.cut(df["DiastolicBloodPressure"], bins=[0, 79, 89, np.inf], labels=["Normal", "Hypertension Stage 1", "Hypertension Stage 2"])
    df["Pregnancies_Category"] = pd.cut(df["Pregnancies"], bins=[0, 4, 10, np.inf], labels=["Low", "Moderate", "High"])

    # One-Hot Encoding for categorical features
    df = pd.get_dummies(df, columns=["Age_Category", "BMI_Category", "Glucose_Category", "BloodPressure_Category", "Pregnancies_Category"], drop_first=False)

    return df

df_new=generate_features(df_wt)
display(df_new.describe())
display(df_new.head())

"""# Check for problematic values if necessary

for column in df_wt.columns:
    has_nan = df_wt[column].isna().any()
    has_inf = np.isinf(df_wt[column]).any()
    print(f"Column '{column}': Has NaN = {has_nan}, Has Inf = {has_inf}")"""

# https://fr.wikipedia.org/wiki/Homeostasic_model_assessment_of_insulin_resistance => Compute HOMA-IR et HOMA-B
# https://pmc.ncbi.nlm.nih.gov/articles/PMC8658352/ => Study with various indicators

# Make global variables for newly generated features for detailed use

new_numerical_features = [
    "Pseudo_Inverse_MI",
    "BMI_DiabetesPedigree",
    "DBP_Age_Freq_Score"
]

new_categorical_features = {
        "Age_Category": ["Age_Category_Young", "Age_Category_Adult", "Age_Category_Senior"],
        "BMI_Category": ["BMI_Category_Normal", "BMI_Category_Overweight", "BMI_Category_Obese"],
        "Glucose_Category": ["Glucose_Category_Hypoglycemia",	"Glucose_Category_Normal", "Glucose_Category_High"],
        "BloodPressure_Category": ["BloodPressure_Category_Normal", "BloodPressure_Category_Hypertension Stage 1", "BloodPressure_Category_Hypertension Stage 2"],
        "Pregnancies_Category": ["Pregnancies_Category_Low", "Pregnancies_Category_Moderate", "Pregnancies_Category_High"]
    }

# Visualizations for newly generated categorical features

def generate_category_summary(df):

    summaries = {}

    for category, subcategories in new_categorical_features.items():
        rows = []
        for subcat in subcategories:
            count = df[subcat].sum()
            diabetic_mean = df.loc[df[subcat] == 1, "Diabetic"].mean()
            rows.append([subcat, count, diabetic_mean])

        summary_df = pd.DataFrame(rows, columns=["Subcategory", "Count", "Diabetic_Mean"])

        # Format styling
        summary_df_styled = (
    summary_df.style
    .set_caption(f"Summary for {category}")
    .format({"Diabetic_Mean": "{:.2%}"})
    .set_properties(**{'text-align': 'center'})
    .set_table_styles([{
        'selector': 'th',
        'props': [('background-color', '#007acc'), ('color', 'white'), ('font-size', '14px')]
    }])
)
        summaries[category] = summary_df_styled

    return summaries

df_target = df_new.copy()
df_target["Diabetic"] = df_noid["Diabetic"]
summaries = generate_category_summary(df_target)
for category, table in summaries.items():
    display(table)
    print("\n" * 1)

# Visualizations for newly generated numerical features

df_num = df_new[new_numerical_features].copy()
df_num["Diabetic"] = df_noid["Diabetic"]

corr_matrix_num = df_num.corr()
plt.figure(figsize=(10, 8))
sns.heatmap(corr_matrix_num, annot=True, cmap='coolwarm', fmt='.2f', linewidths=0.5)
plt.title('Correlation Matrix for New Numerical Features')
plt.show()

# Previsualize all feature importances

from sklearn.feature_selection import mutual_info_classif

def compute__mutual_info(df, target_column):
    """
    Computes and plots the mutual information between features and the target for a given dataframe.
    """
    # Separate features and target
    X = df.drop(columns=target_column, axis=1)
    Y = df[target_column]

    # Compute mutual information
    mut_info = mutual_info_classif(X, Y, random_state=0)

    # Convert to Series for better readability
    mut_info = pd.Series(mut_info, index=X.columns)
    mut_info = mut_info.sort_values(ascending=False)

    # Display results
    print("Mutual Information:")
    print(mut_info)

    # Plot the results
    plt.figure(figsize=(18, 6))
    mut_info.plot.bar()
    plt.title("Mutual Information")
    plt.xlabel("Features")
    plt.ylabel("Mutual Information Score")
    plt.show()

compute__mutual_info(df_target, "Diabetic")

# Make a first drop of poorly relevant features

features_to_remove = [
    'BloodPressure_Category_Hypertension Stage 2',
    'Glucose_Category_High',
    'BloodPressure_Category_Normal',
    'BloodPressure_Category_Hypertension Stage 1',
    'Glucose_Category_Hypoglycemia',
    'Age_Category_Senior',
    'Glucose_Category_Normal',
    'Pregnancies_Category_High',
    'BMI_Category_Obese',
    'Pregnancies_Category_Low',
    'Pregnancies_Category_Moderate'
]

df_target.drop(columns=features_to_remove, inplace=True)

# Skewness handling & Standardization of the data

from sklearn.preprocessing import PowerTransformer

all_numerical_columns=["Pregnancies", "PlasmaGlucose", "DiastolicBloodPressure", "TricepsThickness", "SerumInsulin", "BMI", "DiabetesPedigree", "Age"] + new_numerical_features

def transform_data(df,columns=all_numerical_columns):
    """
    Apply Yeo-Johnson transformation to handle skewness with null values (Pregnancies column) and Standardization to numerical columns of df
    """
    pt = PowerTransformer(method='yeo-johnson', standardize=True)
    df[columns] = pt.fit_transform(df[columns])
    return df

# Compute skewness for all numerical columns in the main DataFrame and compare it with the df_noid skewness
from scipy.stats import skew
skewness_df_noid = df_noid.drop(columns=['Diabetic']).apply(skew, axis=0)

df_clean = transform_data(df_target,columns=all_numerical_columns) # /!\ CLEAN DF AFTER ALL TREATMENTS /!\

skewness_df_clean = df_clean[all_numerical_columns].apply(skew, axis=0)

# Print skewness for each column
print("Skewness for df_noid:")
print(skewness_df_noid)
print("\nSkewness for df_target:")
print(skewness_df_clean)

display(df_clean.head())

nan_counts = df_clean.isna().sum()
inf_counts = np.isinf(df_clean).sum()

print("NaN counts:\n", nan_counts)
print("Infinite values counts:\n", inf_counts)

# https://www.numberanalytics.com/blog/10-statistical-insights-yeo-johnson-transformation-regression-models

"""## Model Selection, Training/Testing & Evaluation"""

# Model selection, training/testing, evaluation, comparison & features importance assessment

import pickle
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import roc_auc_score, roc_curve, classification_report, confusion_matrix
from sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold
from sklearn.feature_selection import RFECV
from sklearn.pipeline import Pipeline
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from xgboost import XGBClassifier
from lightgbm import LGBMClassifier
import warnings

warnings.simplefilter(action='ignore', category=FutureWarning)

# Data preparation
y = df_clean["Diabetic"]
X = df_clean.drop(columns=["Diabetic"])
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)

# Models dictionary with hyperparameters to optimize
models_config = {
    "Logistic Regression": {"model": LogisticRegression(max_iter=1000, class_weight='balanced'), "params": {"C": [0.01, 0.1, 1], "penalty": ["l2"]}},
    "Decision Tree": {"model": DecisionTreeClassifier(random_state=42), "params": {"max_depth": [5, 7], "min_samples_split": [10, 20]}},
    "Random Forest": {"model": RandomForestClassifier(random_state=42, n_jobs=-1, warm_start=True), "params": {"n_estimators": [15], "max_depth": [3], "min_samples_leaf": [10]}},
    "LightGBM": {"model": LGBMClassifier(random_state=42, verbosity=-1), "params": {"n_estimators": [20], "max_depth": [5], "learning_rate": [0.05, 0.1], "reg_alpha": [0.1, 1]}},
    "XGBoost": {"model": XGBClassifier(random_state=42, eval_metric="logloss"), "params": {"n_estimators": [20], "max_depth": [5], "learning_rate": [0.05, 0.1], "gamma": [0, 0.1]}}
}

# Store ROC data for all models
roc_data = []
feature_importances_dict = {}  # Dictionary to store feature importances
best_model = None
best_roc_auc = 0
best_model_name = ""

def train_and_evaluate_model(model, param_grid, Xtrain, Xtest, ytrain, ytest, model_name):
    """Trains the model using GridSearchCV, evaluates it, and stores performance metrics for later plotting."""
    global best_model, best_roc_auc, best_model_name
    rfecv = RFECV(estimator=model, step=1, cv=StratifiedKFold(3), scoring='roc_auc')
    pipeline = Pipeline([
        ('feature_selection', rfecv),
        ('classification', model)
    ])

    param_grid = {f'classification__{key}': value for key, value in param_grid.items()}

    grid_search = GridSearchCV(estimator=pipeline, param_grid=param_grid, cv=StratifiedKFold(3), scoring='roc_auc', n_jobs=-1)
    grid_search.fit(Xtrain, ytrain)

    print(f"\nBest parameters for {model_name}: {grid_search.best_params_}")
    print(f"Best cross-validation score: {grid_search.best_score_}")

    y_pred_proba = grid_search.predict_proba(Xtest)[:, 1]
    y_pred = grid_search.predict(Xtest)
    roc_auc = roc_auc_score(ytest, y_pred_proba)
    print(f"ROC AUC Score on test data for {model_name}: {roc_auc}\n")

    # Store ROC curve data
    fpr, tpr, _ = roc_curve(ytest, y_pred_proba)
    roc_data.append({'model_name': model_name, 'fpr': fpr, 'tpr': tpr, 'auc': roc_auc})

    # Classification report
    print(f"Classification Report for {model_name}:\n")
    print(classification_report(ytest, y_pred))

    # Display confusion matrix
    plt.figure(figsize=(6, 4))
    cm = confusion_matrix(ytest, y_pred)
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['No Diabetes', 'Diabetes'], yticklabels=['No Diabetes', 'Diabetes'])
    plt.xlabel('Predicted')
    plt.ylabel('Actual')
    plt.title(f'Confusion Matrix - {model_name}')
    plt.show()

    # Store feature importances if the model supports it
    if hasattr(grid_search.best_estimator_['classification'], "feature_importances_"):
        feature_importances = grid_search.best_estimator_['classification'].feature_importances_
        feature_importances_dict[model_name] = feature_importances

        # Sort feature importances
        sorted_idx = np.argsort(feature_importances)[::-1]
        sorted_features = X.columns[sorted_idx]  # Get feature names

        # Plot feature importances
        plt.figure(figsize=(10, 6))
        sns.barplot(x=feature_importances[sorted_idx], y=sorted_features, palette="viridis")
        plt.xlabel("Feature Importance")
        plt.ylabel("Features")
        plt.title(f"Feature Importances - {model_name}")
        plt.show()

    # Update best model if the current one has a better ROC AUC score on test data
    if roc_auc > best_roc_auc:
        best_roc_auc = roc_auc
        best_model = grid_search.best_estimator_
        best_model_name = model_name

# Train all models
for name, config in models_config.items():
    print(f"\nTraining model: {name}")
    train_and_evaluate_model(config["model"], config["params"], X_train, X_test, y_train, y_test, name)

# Plot ROC-AUC comparison
plt.figure(figsize=(8, 6))
for model in roc_data:
    plt.plot(model['fpr'], model['tpr'], label=f"{model['model_name']}, AUC={model['auc']:.2f}")

plt.plot([0, 1], [0, 1], color='orange', linestyle='--')
plt.xticks(np.arange(0.0, 1.1, step=0.1))
plt.yticks(np.arange(0.0, 1.1, step=0.1))
plt.xlabel("False Positive Rate", fontsize=12)
plt.ylabel("True Positive Rate", fontsize=12)
plt.title("ROC Curve Comparison", fontweight='bold', fontsize=14)
plt.legend(prop={'size': 10}, loc='lower right')
plt.show()

# Save the best model in Google Drive using pickle module

import pickle

print(f"\nThe best model is: {best_model_name} with ROC AUC of {best_roc_auc}")

drive_path = "/content/drive/My Drive/ML_Models/best_model.pkl"
with open(drive_path, "wb") as f:
    pickle.dump(best_model, f)

print(f"\nBest model saved to '{drive_path}'.")

# Generate a requirements.txt file with the necessary libraries

#!pip freeze > requirements.txt